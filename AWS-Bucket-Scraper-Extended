###
### Author: Jordon Lovik 
### Jordon@loviksecurity.com
###
### AWS Public S3 Bucket Scanner
### 
### This script scans public AWS S3 buckets and generates detailed reports
### of their contents, including URL-encoded links and file extension statistics.
###
### USAGE:
###
### 1. Using the default bucket list defined in the script:
###    python script.py
###
### 2. Scanning a single bucket (auto-detect region):
###    python script.py my-public-bucket
###
### 3. Scanning a single bucket with explicit region:
###    python script.py my-public-bucket us-east-1
###
### 4. Scanning multiple buckets (auto-detect regions):
###    python script.py bucket1 bucket2 bucket3
###
### 5. Scanning multiple buckets with explicit regions:
###    python script.py bucket1 us-east-1 bucket2 us-west-2
###
### 6. Mixed mode - some with regions, some without:
###    python script.py bucket1 bucket2 us-west-2 bucket3 bucket4 eu-central-1
###
### 7. Combining multiple buckets into a single output file:
###    python script.py --combine bucket1 bucket2 bucket3
###    python script.py --combine bucket1 us-east-1 bucket2 us-west-2
###
### Command-line format: [--combine] bucket_name [region] [bucket_name [region] ...]
### - Bucket names can be provided alone (region will be auto-detected)
### - Or provide bucket_name region pairs for explicit region specification
### - You can mix both styles in the same command
### - Use --combine flag to output all results to a single JSON file
###
### OUTPUT:
### - Without --combine: Creates separate JSON files for each bucket scanned
### - With --combine: Creates a single JSON file containing all bucket results
### - Output filename format: BucketName_YYYYMMDD_HHMMSS.json (or Combined_YYYYMMDD_HHMMSS.json)
### - Files include bucket name, region, URL-encoded links, file sizes, and extension statistics
### - Extension statistics are sorted by count (most common first)
###
### EXAMPLES:
### python script.py company-data-bucket
### python script.py bucket1 bucket2 bucket3
### python script.py logs-bucket us-west-2 backups-bucket
### python script.py bucket1 bucket2 us-west-2 bucket3 eu-west-1 bucket4
### python script.py --combine bucket1 bucket2 bucket3
### python script.py --combine bucket1 us-east-1 bucket2 us-west-2
###

import boto3
import botocore
import os
import sys
import json
from urllib.parse import quote
from datetime import datetime

# Default list of public S3 buckets along with their regions
# This will be used if no command-line arguments are provided
# Region can be None to auto-detect
public_buckets = [
    ("Bucket1", None),  # Auto-detect region
    ("Bucket2", "us-west-2"),  # Explicit region
    ("Bucket3", None)  # Auto-detect region
]

def get_bucket_region(bucket_name):
    """
    Automatically detect the region of an S3 bucket.
    Returns the region name or None if detection fails.
    """
    try:
        # Create a client without credentials for public access
        session = boto3.Session()
        config = botocore.config.Config(signature_version=botocore.UNSIGNED)
        
        # Use us-east-1 as the default region for the initial client
        s3 = session.client('s3', region_name='us-east-1', config=config)
        
        # Get the bucket location
        response = s3.get_bucket_location(Bucket=bucket_name)
        
        # The location constraint is None for us-east-1
        region = response['LocationConstraint']
        if region is None:
            region = 'us-east-1'
        
        print(f"Auto-detected region for bucket '{bucket_name}': {region}")
        return region
        
    except botocore.exceptions.ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'NoSuchBucket':
            print(f"Error: Bucket '{bucket_name}' does not exist")
        elif error_code == 'AccessDenied':
            print(f"Error: Access denied when detecting region for bucket '{bucket_name}'")
            print(f"The bucket may be private or you may need to specify the region explicitly")
        else:
            print(f"Error detecting region for bucket '{bucket_name}': {e}")
        return None
    except Exception as e:
        print(f"Unexpected error detecting region for bucket '{bucket_name}': {e}")
        return None

def list_files_in_bucket(bucket_name, region):
    """
    List all files in a bucket and return structured data.
    Returns a dictionary with bucket info, files, and extension statistics.
    """
    result = {
        "bucket_name": bucket_name,
        "region": region,
        "scan_timestamp": datetime.now().isoformat(),
        "files": [],
        "extension_statistics": {},
        "errors": []
    }
    
    try:
        # Create a session without providing credentials
        session = boto3.Session()
        config = botocore.config.Config(signature_version=botocore.UNSIGNED)
        s3 = session.client('s3', region_name=region, config=config)

        # List objects in the bucket
        objects = s3.list_objects_v2(Bucket=bucket_name)

        extension_count = {}  # Dictionary to count file extensions

        for obj in objects.get('Contents', []):
            file_key = obj['Key']
            file_size_bytes = obj['Size']
            file_size_mb = file_size_bytes / (1024 * 1024)
            
            # URL encode the file key to handle spaces and special characters
            encoded_key = quote(file_key, safe='/')
            link = f"https://{bucket_name}.s3.amazonaws.com/{encoded_key}"
            
            # Add file info to results - now includes bucket_name and region
            file_info = {
                "bucket_name": bucket_name,
                "region": region,
                "key": file_key,
                "url": link,
                "size_bytes": file_size_bytes,
                "size_mb": round(file_size_mb, 2),
                "last_modified": obj['LastModified'].isoformat()
            }
            result["files"].append(file_info)
            
            print(f"Bucket: {bucket_name}, Region: {region}, Link: {link}, File: {file_key}, Size: {file_size_mb:.2f} MB")
            
            # Count file extensions
            _, file_extension = os.path.splitext(file_key)
            if file_extension:
                extension_count[file_extension] = extension_count.get(file_extension, 0) + 1

        # Sort extensions by count (descending order)
        sorted_extensions = sorted(extension_count.items(), key=lambda x: x[1], reverse=True)
        result["extension_statistics"] = dict(sorted_extensions)
        
        # Print extension statistics
        for extension, count in sorted_extensions:
            print(f"Bucket: {bucket_name}, Extension: {extension}, Count: {count}")

    except botocore.exceptions.ClientError as e:
        error_msg = ""
        if e.response['Error']['Code'] == 'NoSuchBucket':
            error_msg = f"Bucket {bucket_name} not found in region {region}"
            print(error_msg)
        else:
            error_msg = f"An error occurred in region {region}: {str(e)}"
            print(error_msg)
        result["errors"].append(error_msg)
    
    return result

def is_valid_region(region_str):
    """
    Check if a string looks like a valid AWS region.
    Returns True if it matches the pattern of AWS regions.
    """
    # Common AWS region patterns: us-east-1, eu-west-2, ap-southeast-1, etc.
    valid_prefixes = ['us-', 'eu-', 'ap-', 'sa-', 'ca-', 'me-', 'af-', 'cn-']
    return any(region_str.startswith(prefix) for prefix in valid_prefixes)

def parse_command_line_args():
    """
    Parse command-line arguments for bucket names, optional regions, and combine flag.
    Expected format: [--combine] bucket1 [region1] bucket2 [region2] ...
    Returns a tuple: (combine_flag, list of tuples [(bucket1, region1), ...])
    Region will be None if not provided (for auto-detection)
    
    This function intelligently handles mixed input:
    - bucket1 bucket2 bucket3 (all auto-detect)
    - bucket1 us-east-1 bucket2 us-west-2 (all explicit)
    - bucket1 bucket2 us-west-2 bucket3 (mixed: bucket1 auto, bucket2 explicit, bucket3 auto)
    """
    if len(sys.argv) < 2:
        return False, None  # No command-line arguments provided
    
    args = sys.argv[1:]
    combine = False
    
    # Check for --combine flag
    if args[0] == '--combine':
        combine = True
        args = args[1:]  # Remove the flag from args
    
    if len(args) == 0:
        return combine, None
    
    # Parse arguments - can be bucket names alone or bucket-region pairs
    buckets = []
    i = 0
    while i < len(args):
        bucket_name = args[i]
        region = None
        
        # Check if the next argument exists and is a region
        if i + 1 < len(args) and is_valid_region(args[i + 1]):
            region = args[i + 1]
            i += 2  # Skip both bucket and region
        else:
            i += 1  # Skip just the bucket, region will be auto-detected
        
        buckets.append((bucket_name, region))
    
    return combine, buckets

if __name__ == '__main__':
    # Try to get buckets from command-line arguments
    combine_mode, cmd_buckets = parse_command_line_args()
    
    # Use command-line buckets if provided, otherwise use default list
    if cmd_buckets:
        buckets_to_process = cmd_buckets
        print(f"Using {len(buckets_to_process)} bucket(s) from command-line arguments")
        print(f"Combine mode: {'ON' if combine_mode else 'OFF'}\n")
    else:
        buckets_to_process = public_buckets
        print(f"Using {len(buckets_to_process)} bucket(s) from default list")
        print(f"Combine mode: {'ON' if combine_mode else 'OFF'}\n")
    
    # Auto-detect regions where needed
    processed_buckets = []
    for bucket, region in buckets_to_process:
        if region is None:
            print(f"Auto-detecting region for bucket: {bucket}")
            detected_region = get_bucket_region(bucket)
            if detected_region is None:
                print(f"Skipping bucket '{bucket}' - could not detect region\n")
                continue
            processed_buckets.append((bucket, detected_region))
        else:
            print(f"Using explicit region '{region}' for bucket: {bucket}")
            processed_buckets.append((bucket, region))
    
    if not processed_buckets:
        print("No valid buckets to process. Exiting.")
        sys.exit(1)
    
    print()  # Blank line for readability
    
    if combine_mode:
        # COMBINE MODE: All buckets in one JSON file
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f'Combined_{timestamp}.json'
        
        combined_results = {
            "scan_timestamp": datetime.now().isoformat(),
            "total_buckets": len(processed_buckets),
            "buckets": []
        }
        
        for bucket, region in processed_buckets:
            print(f"\nScanning bucket: {bucket} in region: {region}")
            print("=" * 80)
            bucket_result = list_files_in_bucket(bucket, region)
            combined_results["buckets"].append(bucket_result)
            print(f"Completed scanning {bucket}\n")
        
        # Write combined results to single JSON file
        with open(output_file, 'w') as f:
            json.dump(combined_results, f, indent=2)
        
        print(f"\n{'=' * 80}")
        print(f"All results saved to: {output_file}")
        print(f"Total buckets scanned: {len(processed_buckets)}")
        
    else:
        # SEPARATE MODE: Individual JSON file per bucket
        for bucket, region in processed_buckets:
            print(f"\nScanning bucket: {bucket} in region: {region}")
            print("=" * 80)
            
            # Generate unique output filename for each bucket with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'{bucket}_{timestamp}.json'
            
            # Scan the bucket
            bucket_result = list_files_in_bucket(bucket, region)
            
            # Write results to JSON file
            with open(output_file, 'w') as f:
                json.dump(bucket_result, f, indent=2)
            
            print(f"\nCompleted scanning {bucket}. Results saved to {output_file}\n")
